#网络模型描述文件
net: "E:/caffe-windows/examples/mnist/lenet_train_test.prototxt"

#这个要与test layer中的batch_size结合起来理解。数据集测试样本总数为10000，一次性执行全部数据效率很低，
#因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。
#因此test_iter设置为100。执行完一次全部数据，称之为一个epoch
test_iter: 100
#每训练500次进行一次测试
test_interval: 500

#基础学习率
base_lr: 0.01
#学习率调整策略
#如果设置为inv,还需要设置一个power, 返回base_lr *(1 + gamma * iter)^(- power)
#其中iter表示当前的迭代次数
lr_policy: "inv"
gamma: 0.0001
power: 0.75

#学习率变化频率,每10000次的迭代后降低学习率：乘以gamma
stepsize:10000

#动力
momentum: 0.9
#type:SGD #优化算法的选择。默认值就是SGD，Caffe中一共有6中优化算法可以选择
#Stochastic Gradient Descent (type: "SGD"), 在Caffe中SGD其实应该是Momentum
#AdaDelta (type: "AdaDelta"),
#Adaptive Gradient (type: "AdaGrad"),
#Adam (type: "Adam"),
#Nesterov’s Accelerated Gradient (type: "Nesterov")
#RMSprop (type: "RMSProp")

#权重衰减项，其实也就是正则化项。作用是防止过拟合
weight_decay: 0.0005

#每训练100次屏幕上显示一次，如果设置为0则不显示
display: 100
#最大迭代次数，这个数设置太小，会导致没有收敛，精确度很低。设置太大，会导致震荡，浪费时间
max_iter: 20000

#快照，可以把训练的model和solver的状态进行保存。
#每迭代5000次保存一次，如果设置为0则不保存
snapshot: 5000
snapshot_prefix: "E:/caffe-windows/examples/mnist/models"

#选择运行模式
solver_mode: GPU

